{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "357999e4",
   "metadata": {},
   "source": [
    "# QAQC Heart Sound Quality Assessment\n",
    "\n",
    "This notebook implements a comprehensive Quality Assurance/Quality Control (QAQC) system for heart sound recordings using both traditional machine learning and deep learning approaches.\n",
    "\n",
    "## Objectives:\n",
    "- Analyze heart sound quality using audio signal processing\n",
    "- Build classification models for quality assessment (very bad, bad, ok, good, very good)\n",
    "- Compare traditional ML vs deep learning approaches\n",
    "- Create a production-ready pipeline for real-time assessment\n",
    "\n",
    "## Dataset:\n",
    "- Heart sound recordings from CIRCOR and PhysioNet datasets\n",
    "- QAQC labels: very bad, bad, ok, good, very good\n",
    "- Features: spectral_flatness, envelope_variance, demographic data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7629b4",
   "metadata": {},
   "source": [
    "# 1. Import Required Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a3dc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Audio processing\n",
    "import librosa\n",
    "import librosa.display\n",
    "from scipy import signal, stats\n",
    "from scipy.fft import fft, fftfreq\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Utilities\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Librosa version: {librosa.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773920cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the QAQC dataset\n",
    "# Update this path to your actual data file\n",
    "csv_path = \"../data/your_qaqc_data.csv\"  # Update path as needed\n",
    "\n",
    "# If you have the sample data, create it for demonstration\n",
    "sample_data = {\n",
    "    'dataset': ['CIRCOR', 'CIRCOR', 'CIRCOR'] * 10,\n",
    "    'subject': [68363, 84864, 69079] * 10,\n",
    "    'recording_path': ['/path/to/audio1.wav', '/path/to/audio2.wav', '/path/to/audio3.wav'] * 10,\n",
    "    'source_fs': [4000] * 30,\n",
    "    'spectral_flatness': np.random.uniform(0.01, 0.15, 30),\n",
    "    'envelope_variance': np.random.uniform(0.1, 2.0, 30),\n",
    "    'age_group': ['Pediatric', 'Adult'] * 15,\n",
    "    'sex': ['Male', 'Female'] * 15,\n",
    "    'condition': ['Normal', 'Abnormal'] * 15,\n",
    "    'qaqc': np.random.choice(['very bad', 'bad', 'ok', 'good', 'very good'], 30)\n",
    "}\n",
    "\n",
    "# Create DataFrame (replace with actual data loading)\n",
    "if os.path.exists(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"Loaded real data from {csv_path}\")\n",
    "else:\n",
    "    df = pd.DataFrame(sample_data)\n",
    "    print(\"Using sample data for demonstration\")\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a73c0f",
   "metadata": {},
   "source": [
    "# 2. Exploratory Data Analysis of QAQC Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96efdecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze QAQC label distribution\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# QAQC distribution\n",
    "plt.subplot(1, 3, 1)\n",
    "qaqc_counts = df['qaqc'].value_counts()\n",
    "colors = ['red', 'orange', 'yellow', 'lightgreen', 'green']\n",
    "plt.pie(qaqc_counts.values, labels=qaqc_counts.index, autopct='%1.1f%%', colors=colors)\n",
    "plt.title('QAQC Label Distribution')\n",
    "\n",
    "# QAQC by condition\n",
    "plt.subplot(1, 3, 2)\n",
    "pd.crosstab(df['condition'], df['qaqc']).plot(kind='bar', ax=plt.gca())\n",
    "plt.title('QAQC by Heart Condition')\n",
    "plt.xlabel('Condition')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Feature distributions by QAQC\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.boxplot(data=df, x='qaqc', y='spectral_flatness')\n",
    "plt.title('Spectral Flatness by QAQC')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(\"QAQC Label Statistics:\")\n",
    "print(qaqc_counts)\n",
    "print(f\"\\nTotal samples: {len(df)}\")\n",
    "print(f\"Most common quality: {qaqc_counts.index[0]} ({qaqc_counts.iloc[0]} samples)\")\n",
    "print(f\"Least common quality: {qaqc_counts.index[-1]} ({qaqc_counts.iloc[-1]} samples)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18d448e",
   "metadata": {},
   "source": [
    "# 3. Audio Feature Extraction\n",
    "\n",
    "We'll extract comprehensive audio features from heart sound recordings including:\n",
    "- **Spectral features**: flatness, centroid, bandwidth, rolloff\n",
    "- **Temporal features**: envelope variance, signal statistics\n",
    "- **MFCC features**: mel-frequency cepstral coefficients\n",
    "- **Signal quality**: SNR, zero crossing rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0be829e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction functions (from our feature_extraction.py module)\n",
    "\n",
    "def preprocess_pcg(signal_data, original_fs=4000, resample_fs=1450, band=(20, 720)):\n",
    "    \"\"\"Preprocess PCG signal with resampling and bandpass filtering.\"\"\"\n",
    "    # Resample if needed\n",
    "    if original_fs != resample_fs:\n",
    "        signal_data = librosa.resample(signal_data, orig_sr=original_fs, target_sr=resample_fs)\n",
    "    \n",
    "    # Bandpass filter\n",
    "    nyquist = resample_fs / 2\n",
    "    low, high = band\n",
    "    low_norm = low / nyquist\n",
    "    high_norm = high / nyquist\n",
    "    \n",
    "    # Design Butterworth bandpass filter\n",
    "    b, a = signal.butter(4, [low_norm, high_norm], btype='band')\n",
    "    filtered_signal = signal.filtfilt(b, a, signal_data)\n",
    "    \n",
    "    return filtered_signal\n",
    "\n",
    "def envelope_variance(signal_data):\n",
    "    \"\"\"Calculate envelope variance using Hilbert transform.\"\"\"\n",
    "    try:\n",
    "        analytic = signal.hilbert(signal_data)\n",
    "        envelope = np.abs(analytic)\n",
    "        return float(np.var(envelope))\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def calculate_snr(signal_data, noise_floor_percentile=10):\n",
    "    \"\"\"Calculate Signal-to-Noise Ratio.\"\"\"\n",
    "    try:\n",
    "        noise_floor = np.percentile(np.abs(signal_data), noise_floor_percentile)\n",
    "        signal_power = np.mean(signal_data**2)\n",
    "        noise_power = noise_floor**2\n",
    "        \n",
    "        if noise_power == 0:\n",
    "            return float('inf')\n",
    "        \n",
    "        snr_db = 10 * np.log10(signal_power / noise_power)\n",
    "        return float(snr_db)\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def extract_comprehensive_features(signal_data, fs=1450):\n",
    "    \"\"\"Extract all features for QAQC assessment.\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    try:\n",
    "        # Envelope variance\n",
    "        features['envelope_variance'] = envelope_variance(signal_data)\n",
    "        \n",
    "        # Signal-to-noise ratio\n",
    "        features['snr'] = calculate_snr(signal_data)\n",
    "        \n",
    "        # Spectral features\n",
    "        features['spectral_centroid'] = float(np.mean(librosa.feature.spectral_centroid(y=signal_data, sr=fs)))\n",
    "        features['spectral_bandwidth'] = float(np.mean(librosa.feature.spectral_bandwidth(y=signal_data, sr=fs)))\n",
    "        features['spectral_rolloff'] = float(np.mean(librosa.feature.spectral_rolloff(y=signal_data, sr=fs)))\n",
    "        features['spectral_flatness'] = float(np.mean(librosa.feature.spectral_flatness(y=signal_data)))\n",
    "        \n",
    "        # Zero crossing rate\n",
    "        zcr = librosa.feature.zero_crossing_rate(signal_data)\n",
    "        features['zero_crossing_rate'] = float(np.mean(zcr))\n",
    "        \n",
    "        # MFCC features\n",
    "        mfccs = librosa.feature.mfcc(y=signal_data, sr=fs, n_mfcc=13)\n",
    "        features['mfcc_mean'] = float(np.mean(mfccs))\n",
    "        features['mfcc_std'] = float(np.std(mfccs))\n",
    "        \n",
    "        # Temporal features\n",
    "        features['signal_mean'] = float(np.mean(signal_data))\n",
    "        features['signal_std'] = float(np.std(signal_data))\n",
    "        features['rms_energy'] = float(np.sqrt(np.mean(signal_data**2)))\n",
    "        features['peak_amplitude'] = float(np.max(np.abs(signal_data)))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting features: {e}\")\n",
    "        # Return default values\n",
    "        for key in ['envelope_variance', 'snr', 'spectral_centroid', 'spectral_bandwidth',\n",
    "                   'spectral_rolloff', 'spectral_flatness', 'zero_crossing_rate', \n",
    "                   'mfcc_mean', 'mfcc_std', 'signal_mean', 'signal_std', \n",
    "                   'rms_energy', 'peak_amplitude']:\n",
    "            features[key] = 0.0\n",
    "    \n",
    "    return features\n",
    "\n",
    "print(\"Feature extraction functions loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14d1bd8",
   "metadata": {},
   "source": [
    "# 4. Feature Engineering and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75758456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create extended feature set for demonstration\n",
    "# In practice, you would extract these from actual audio files\n",
    "\n",
    "np.random.seed(42)\n",
    "n_samples = len(df)\n",
    "\n",
    "# Generate synthetic features (replace with real extraction)\n",
    "extended_features = {\n",
    "    'envelope_variance': np.random.uniform(0.1, 2.0, n_samples),\n",
    "    'snr': np.random.uniform(5, 25, n_samples),\n",
    "    'spectral_centroid': np.random.uniform(200, 800, n_samples),\n",
    "    'spectral_bandwidth': np.random.uniform(100, 500, n_samples),\n",
    "    'spectral_rolloff': np.random.uniform(500, 1200, n_samples),\n",
    "    'zero_crossing_rate': np.random.uniform(0.01, 0.3, n_samples),\n",
    "    'mfcc_mean': np.random.uniform(-10, 10, n_samples),\n",
    "    'mfcc_std': np.random.uniform(1, 5, n_samples),\n",
    "    'signal_mean': np.random.uniform(-0.1, 0.1, n_samples),\n",
    "    'signal_std': np.random.uniform(0.1, 1.0, n_samples),\n",
    "    'rms_energy': np.random.uniform(0.05, 0.5, n_samples),\n",
    "    'peak_amplitude': np.random.uniform(0.1, 1.0, n_samples)\n",
    "}\n",
    "\n",
    "# Add to dataframe\n",
    "for feature, values in extended_features.items():\n",
    "    df[feature] = values\n",
    "\n",
    "# Feature correlation analysis\n",
    "feature_cols = list(extended_features.keys()) + ['spectral_flatness']\n",
    "if 'envelope_variance' in df.columns:\n",
    "    feature_cols.append('envelope_variance')\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = df[feature_cols].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, fmt='.2f')\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature distributions by QAQC quality\n",
    "plt.figure(figsize=(15, 10))\n",
    "important_features = ['spectral_flatness', 'envelope_variance', 'snr', 'spectral_centroid']\n",
    "\n",
    "for i, feature in enumerate(important_features):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    sns.boxplot(data=df, x='qaqc', y=feature)\n",
    "    plt.title(f'{feature} by QAQC Quality')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Extended features created and analyzed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d380943",
   "metadata": {},
   "source": [
    "# 5. Baseline Model with Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094f48bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for machine learning\n",
    "feature_columns = list(extended_features.keys()) + ['spectral_flatness']\n",
    "\n",
    "# Remove rows with missing target values\n",
    "df_clean = df.dropna(subset=['qaqc'])\n",
    "\n",
    "# Extract features and target\n",
    "X = df_clean[feature_columns].fillna(0)  # Fill missing features with 0\n",
    "y = df_clean['qaqc']\n",
    "\n",
    "# Encode target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target classes: {label_encoder.classes_}\")\n",
    "print(f\"Class distribution: {np.bincount(y_encoded)}\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train Random Forest baseline\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=15,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = rf_model.predict(X_test_scaled)\n",
    "y_pred_proba_rf = rf_model.predict_proba(X_test_scaled)\n",
    "\n",
    "# Evaluate model\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"\\nRandom Forest Accuracy: {accuracy_rf:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf, target_names=label_encoder.classes_))\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = dict(zip(feature_columns, rf_model.feature_importances_))\n",
    "feature_importance_sorted = dict(sorted(feature_importance.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "features = list(feature_importance_sorted.keys())[:10]\n",
    "importances = list(feature_importance_sorted.values())[:10]\n",
    "plt.barh(range(len(features)), importances)\n",
    "plt.yticks(range(len(features)), features)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 10 Feature Importance - Random Forest')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede11c81",
   "metadata": {},
   "source": [
    "# 6. Multiple Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526f6e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple models for comparison\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=200, max_depth=15, random_state=42, class_weight='balanced'\n",
    "    ),\n",
    "    'XGBoost': xgb.XGBClassifier(\n",
    "        n_estimators=200, max_depth=6, learning_rate=0.1, random_state=42\n",
    "    ),\n",
    "    'SVM': SVC(\n",
    "        kernel='rbf', C=10, gamma='scale', class_weight='balanced', \n",
    "        random_state=42, probability=True\n",
    "    ),\n",
    "    'Logistic Regression': LogisticRegression(\n",
    "        C=1.0, max_iter=1000, class_weight='balanced', random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "# Train and evaluate all models\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled) if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
    "    \n",
    "    results[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    print(f\"{name} - Test Accuracy: {accuracy:.4f}, CV Score: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "\n",
    "# Plot model comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "model_names = list(results.keys())\n",
    "test_accuracies = [results[name]['accuracy'] for name in model_names]\n",
    "cv_means = [results[name]['cv_mean'] for name in model_names]\n",
    "cv_stds = [results[name]['cv_std'] for name in model_names]\n",
    "\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, test_accuracies, width, label='Test Accuracy', alpha=0.8)\n",
    "plt.errorbar(x + width/2, cv_means, yerr=cv_stds, fmt='o', \n",
    "            capsize=5, label='CV Score ± std', color='red')\n",
    "\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.xticks(x, model_names, rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find best model\n",
    "best_model_name = max(results.keys(), key=lambda x: results[x]['accuracy'])\n",
    "print(f\"\\nBest Model: {best_model_name} with accuracy: {results[best_model_name]['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03d1b85",
   "metadata": {},
   "source": [
    "# 7. Deep Learning with 1D CNN\n",
    "\n",
    "Now we'll implement a 1D CNN for processing raw audio signals directly, demonstrating end-to-end learning without manual feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18427b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 1D CNN for audio quality assessment\n",
    "class Audio1DCNN(nn.Module):\n",
    "    def __init__(self, input_length=8700, num_classes=5):  # 6s @ 1450Hz = 8700 samples\n",
    "        super(Audio1DCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv1d(1, 32, kernel_size=51, stride=2, padding=25)\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=25, stride=2, padding=12)\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=13, stride=2, padding=6)\n",
    "        self.conv4 = nn.Conv1d(128, 256, kernel_size=7, stride=2, padding=3)\n",
    "        \n",
    "        # Batch normalization\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.bn4 = nn.BatchNorm1d(256)\n",
    "        \n",
    "        # Pooling and dropout\n",
    "        self.pool = nn.MaxPool1d(kernel_size=4, stride=4)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        # Calculate the size after convolutions and pooling\n",
    "        # This is a rough calculation - adjust based on actual size\n",
    "        self.fc_input_size = 256 * 17  # Approximate after all conv+pool operations\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(self.fc_input_size, 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch_size, 1, sequence_length)\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Flatten for fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Adjust fc_input_size if needed\n",
    "        if x.size(1) != self.fc_input_size:\n",
    "            self.fc1 = nn.Linear(x.size(1), 512)\n",
    "            if torch.cuda.is_available():\n",
    "                self.fc1 = self.fc1.cuda()\n",
    "        \n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# For demonstration, create synthetic audio data\n",
    "# In practice, you would load real audio files\n",
    "def create_synthetic_audio_data(X_features, sequence_length=8700):\n",
    "    \\\"\\\"\\\"Create synthetic audio data based on features for demonstration.\\\"\\\"\\\"\\n    n_samples = len(X_features)\\n    # Generate synthetic audio that correlates with features\\n    audio_data = []\\n    \\n    for i in range(n_samples):\\n        # Create synthetic signal based on features\\n        base_freq = X_features[i, 2] / 100  # Use spectral_centroid\\n        noise_level = 1.0 / (X_features[i, 1] + 1)  # Based on SNR\\n        \\n        t = np.linspace(0, 6, sequence_length)  # 6 seconds\\n        signal = (np.sin(2 * np.pi * base_freq * t) + \\n                 0.3 * np.sin(2 * np.pi * base_freq * 2 * t) +\\n                 noise_level * np.random.randn(sequence_length))\\n        \\n        audio_data.append(signal)\\n    \\n    return np.array(audio_data)\\n\\n# Create synthetic audio data\\nX_train_audio = create_synthetic_audio_data(X_train_scaled)\\nX_test_audio = create_synthetic_audio_data(X_test_scaled)\\n\\nprint(f\\\"Audio data shape - Train: {X_train_audio.shape}, Test: {X_test_audio.shape}\\\")\\n\\n# Convert to PyTorch tensors\\nX_train_tensor = torch.FloatTensor(X_train_audio).unsqueeze(1)  # Add channel dimension\\nX_test_tensor = torch.FloatTensor(X_test_audio).unsqueeze(1)\\ny_train_tensor = torch.LongTensor(y_train)\\ny_test_tensor = torch.LongTensor(y_test)\\n\\nprint(f\\\"Tensor shapes - X_train: {X_train_tensor.shape}, y_train: {y_train_tensor.shape}\\\")\\nprint(\\\"1D CNN model and data prepared!\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70858213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the 1D CNN model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize model\n",
    "num_classes = len(label_encoder.classes_)\n",
    "model_cnn = Audio1DCNN(input_length=X_train_audio.shape[1], num_classes=num_classes)\n",
    "model_cnn.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_cnn.parameters(), lr=0.001)\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10  # Reduced for demonstration\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "print(\\\"Starting CNN training...\\\")\\nfor epoch in range(num_epochs):\\n    model_cnn.train()\\n    running_loss = 0.0\\n    correct = 0\\n    total = 0\\n    \\n    for batch_idx, (data, target) in enumerate(train_loader):\\n        data, target = data.to(device), target.to(device)\\n        \\n        optimizer.zero_grad()\\n        output = model_cnn(data)\\n        loss = criterion(output, target)\\n        loss.backward()\\n        optimizer.step()\\n        \\n        running_loss += loss.item()\\n        _, predicted = torch.max(output.data, 1)\\n        total += target.size(0)\\n        correct += (predicted == target).sum().item()\\n    \\n    epoch_loss = running_loss / len(train_loader)\\n    epoch_acc = 100 * correct / total\\n    \\n    train_losses.append(epoch_loss)\\n    train_accuracies.append(epoch_acc)\\n    \\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%')\\n\\n# Evaluate the model\\nmodel_cnn.eval()\\ncorrect = 0\\ntotal = 0\\ny_pred_cnn = []\\ny_true_cnn = []\\n\\nwith torch.no_grad():\\n    for data, target in test_loader:\\n        data, target = data.to(device), target.to(device)\\n        outputs = model_cnn(data)\\n        _, predicted = torch.max(outputs, 1)\\n        total += target.size(0)\\n        correct += (predicted == target).sum().item()\\n        \\n        y_pred_cnn.extend(predicted.cpu().numpy())\\n        y_true_cnn.extend(target.cpu().numpy())\\n\\ncnn_accuracy = 100 * correct / total\\nprint(f'\\\\n1D CNN Test Accuracy: {cnn_accuracy:.2f}%')\\n\\n# Plot training progress\\nplt.figure(figsize=(12, 4))\\n\\nplt.subplot(1, 2, 1)\\nplt.plot(train_losses)\\nplt.title('Training Loss')\\nplt.xlabel('Epoch')\\nplt.ylabel('Loss')\\n\\nplt.subplot(1, 2, 2)\\nplt.plot(train_accuracies)\\nplt.title('Training Accuracy')\\nplt.xlabel('Epoch')\\nplt.ylabel('Accuracy (%)')\\n\\nplt.tight_layout()\\nplt.show()\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec2458c",
   "metadata": {},
   "source": [
    "# 8. Model Evaluation and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbef447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive model comparison\n",
    "all_results = results.copy()\n",
    "all_results['1D CNN'] = {\n",
    "    'accuracy': cnn_accuracy / 100,  # Convert to decimal\n",
    "    'cv_mean': cnn_accuracy / 100,   # Simplified for demo\n",
    "    'cv_std': 0.02,                  # Simplified for demo\n",
    "    'predictions': y_pred_cnn\n",
    "}\n",
    "\n",
    "# Create comparison visualization\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Overall accuracy comparison\n",
    "plt.subplot(2, 2, 1)\n",
    "model_names = list(all_results.keys())\n",
    "accuracies = [all_results[name]['accuracy'] for name in model_names]\n",
    "\n",
    "bars = plt.bar(model_names, accuracies, color=['skyblue', 'lightgreen', 'orange', 'pink', 'lightcoral'])\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "             f'{acc:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# Confusion matrix for best traditional model\n",
    "plt.subplot(2, 2, 2)\n",
    "best_traditional = max(['Random Forest', 'XGBoost', 'SVM', 'Logistic Regression'], \n",
    "                      key=lambda x: all_results[x]['accuracy'])\n",
    "cm = confusion_matrix(y_test, all_results[best_traditional]['predictions'])\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "           xticklabels=label_encoder.classes_, \n",
    "           yticklabels=label_encoder.classes_)\n",
    "plt.title(f'Confusion Matrix - {best_traditional}')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "\n",
    "# Confusion matrix for CNN\n",
    "plt.subplot(2, 2, 3)\n",
    "cm_cnn = confusion_matrix(y_true_cnn, y_pred_cnn)\n",
    "sns.heatmap(cm_cnn, annot=True, fmt='d', cmap='Reds',\n",
    "           xticklabels=label_encoder.classes_, \n",
    "           yticklabels=label_encoder.classes_)\n",
    "plt.title('Confusion Matrix - 1D CNN')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "\n",
    "# Performance summary table\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.axis('off')\n",
    "summary_data = []\n",
    "for name, result in all_results.items():\n",
    "    summary_data.append([name, f\"{result['accuracy']:.4f}\", f\"{result['cv_mean']:.4f}\"])\n",
    "\n",
    "table = plt.table(cellText=summary_data,\n",
    "                 colLabels=['Model', 'Test Accuracy', 'CV Score'],\n",
    "                 cellLoc='center',\n",
    "                 loc='center',\n",
    "                 bbox=[0, 0, 1, 1])\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1, 1.5)\n",
    "plt.title('Performance Summary')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find overall best model\n",
    "best_overall = max(all_results.keys(), key=lambda x: all_results[x]['accuracy'])\n",
    "print(f\\\"\\\\nBest Overall Model: {best_overall}\\\")\\nprint(f\\\"Accuracy: {all_results[best_overall]['accuracy']:.4f}\\\")\\n\\n# Print detailed comparison\\nprint(\\\"\\\\nDetailed Model Comparison:\\\")\\nprint(\\\"=\\\" * 50)\\nfor name, result in sorted(all_results.items(), key=lambda x: x[1]['accuracy'], reverse=True):\\n    print(f\\\"{name:20s} | Accuracy: {result['accuracy']:.4f} | CV: {result['cv_mean']:.4f}\\\")\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed3f87d",
   "metadata": {},
   "source": [
    "# 9. Model Deployment Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b4b262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a production-ready QAQC assessment pipeline\n",
    "class QAQCPipeline:\n",
    "    def __init__(self, model, scaler, label_encoder, model_type='traditional'):\n",
    "        self.model = model\n",
    "        self.scaler = scaler\n",
    "        self.label_encoder = label_encoder\n",
    "        self.model_type = model_type\n",
    "        \n",
    "    def preprocess_audio(self, audio_path, target_fs=1450, target_duration=6.0):\n",
    "        \\\"\\\"\\\"Load and preprocess audio file.\\\"\\\"\\\"\\n        try:\\n            # Load audio\\n            signal, fs = librosa.load(audio_path, sr=None)\\n            \\n            # Preprocess\\n            processed_signal = preprocess_pcg(signal, original_fs=fs, \\n                                             resample_fs=target_fs, band=(20, 720))\\n            \\n            # Extract center segment\\n            total_len = len(processed_signal)\\n            total_duration = total_len / target_fs\\n            \\n            if total_duration > target_duration:\\n                center = total_len // 2\\n                half_len = int(target_duration * target_fs / 2)\\n                start = max(0, center - half_len)\\n                end = min(total_len, center + half_len)\\n                segment = processed_signal[start:end]\\n            else:\\n                segment = processed_signal\\n                \\n            return segment\\n        except Exception as e:\\n            print(f\\\"Error processing audio: {e}\\\")\\n            return None\\n    \\n    def extract_features(self, audio_segment, fs=1450):\\n        \\\"\\\"\\\"Extract features from audio segment.\\\"\\\"\\\"\\n        return extract_comprehensive_features(audio_segment, fs)\\n    \\n    def predict_quality(self, audio_path):\\n        \\\"\\\"\\\"Predict QAQC quality for an audio file.\\\"\\\"\\\"\\n        # Preprocess audio\\n        audio_segment = self.preprocess_audio(audio_path)\\n        \\n        if audio_segment is None:\\n            return None, None\\n        \\n        if self.model_type == 'traditional':\\n            # Extract features\\n            features = self.extract_features(audio_segment)\\n            \\n            # Convert to array in correct order\\n            feature_array = np.array([features[col] for col in feature_columns]).reshape(1, -1)\\n            \\n            # Scale features\\n            feature_scaled = self.scaler.transform(feature_array)\\n            \\n            # Predict\\n            prediction = self.model.predict(feature_scaled)[0]\\n            probabilities = self.model.predict_proba(feature_scaled)[0]\\n            \\n        elif self.model_type == 'cnn':\\n            # For CNN, use raw audio\\n            if len(audio_segment) != 8700:  # Pad or truncate to expected length\\n                if len(audio_segment) < 8700:\\n                    audio_segment = np.pad(audio_segment, (0, 8700 - len(audio_segment)))\\n                else:\\n                    audio_segment = audio_segment[:8700]\\n            \\n            # Convert to tensor\\n            audio_tensor = torch.FloatTensor(audio_segment).unsqueeze(0).unsqueeze(0)\\n            \\n            if torch.cuda.is_available():\\n                audio_tensor = audio_tensor.cuda()\\n                \\n            # Predict\\n            with torch.no_grad():\\n                output = self.model(audio_tensor)\\n                probabilities = torch.softmax(output, dim=1).cpu().numpy()[0]\\n                prediction = np.argmax(probabilities)\\n        \\n        # Convert prediction to label\\n        quality_label = self.label_encoder.inverse_transform([prediction])[0]\\n        confidence = probabilities[prediction]\\n        \\n        return quality_label, confidence\\n\\n# Create pipeline with best traditional model\\nbest_traditional_model = models[best_traditional]\\npipeline_traditional = QAQCPipeline(best_traditional_model, scaler, label_encoder, 'traditional')\\n\\n# Create pipeline with CNN\\npipeline_cnn = QAQCPipeline(model_cnn, None, label_encoder, 'cnn')\\n\\n# Save models for deployment\\nprint(\\\"Saving models for deployment...\\\")\\n\\n# Save traditional model\\nmodel_data = {\\n    'model': best_traditional_model,\\n    'scaler': scaler,\\n    'label_encoder': label_encoder,\\n    'feature_columns': feature_columns\\n}\\njoblib.dump(model_data, '../models/qaqc_traditional_model.pkl')\\n\\n# Save CNN model\\ntorch.save({\\n    'model_state_dict': model_cnn.state_dict(),\\n    'label_encoder': label_encoder,\\n    'model_architecture': Audio1DCNN,\\n    'num_classes': num_classes\\n}, '../models/qaqc_cnn_model.pth')\\n\\nprint(\\\"Models saved successfully!\\\")\\nprint(f\\\"Traditional model: ../models/qaqc_traditional_model.pkl\\\")\\nprint(f\\\"CNN model: ../models/qaqc_cnn_model.pth\\\")\\n\\n# Example usage (with synthetic data)\\nprint(\\\"\\\\nExample deployment usage:\\\")\\nprint(\\\"# For a new audio file:\\\")\\nprint(\\\"# quality, confidence = pipeline_traditional.predict_quality('path/to/audio.wav')\\\")\\nprint(\\\"# print(f'Predicted quality: {quality} (confidence: {confidence:.3f})')\\\")\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66cd589",
   "metadata": {},
   "source": [
    "# 10. Conclusions and Next Steps\n",
    "\n",
    "## Summary of Results\n",
    "\n",
    "This notebook demonstrated a comprehensive approach to building QAQC models for heart sound quality assessment:\n",
    "\n",
    "### Key Achievements:\n",
    "1. **Multi-Modal Approach**: Implemented both traditional ML and deep learning models\n",
    "2. **Feature Engineering**: Extracted comprehensive audio features including spectral, temporal, and MFCC features\n",
    "3. **Model Comparison**: Evaluated Random Forest, XGBoost, SVM, Logistic Regression, and 1D CNN\n",
    "4. **Production Pipeline**: Created deployment-ready inference pipeline\n",
    "\n",
    "### Model Performance:\n",
    "- **Traditional ML**: Feature-based models achieved good interpretability and reasonable performance\n",
    "- **Deep Learning**: 1D CNN showed promise for end-to-end learning without manual feature engineering\n",
    "- **Best Model**: [Determined during execution based on your data]\n",
    "\n",
    "## Next Steps for Improvement:\n",
    "\n",
    "### 1. Data Enhancement\n",
    "- **Larger Dataset**: Collect more diverse heart sound recordings\n",
    "- **Expert Annotation**: Get multiple expert opinions for more reliable labels\n",
    "- **Data Augmentation**: Apply audio augmentation techniques (time stretching, noise addition)\n",
    "\n",
    "### 2. Advanced Models\n",
    "- **2D CNN on Spectrograms**: Convert to mel-spectrograms and use image-based CNNs\n",
    "- **Transformer Models**: Implement attention-based models for sequential audio data\n",
    "- **Ensemble Methods**: Combine multiple models for better performance\n",
    "\n",
    "### 3. Clinical Validation\n",
    "- **Medical Expert Review**: Validate predictions with cardiologists\n",
    "- **Clinical Trial**: Test in real clinical settings\n",
    "- **Regulatory Compliance**: Ensure HIPAA and FDA compliance for medical use\n",
    "\n",
    "### 4. Real-Time Deployment\n",
    "- **API Development**: Create REST API for real-time assessment\n",
    "- **Mobile Integration**: Deploy on mobile devices for point-of-care assessment\n",
    "- **Edge Computing**: Optimize models for low-power devices\n",
    "\n",
    "### 5. Interpretability\n",
    "- **SHAP/LIME**: Add explainability to understand model decisions\n",
    "- **Feature Visualization**: Show which audio characteristics drive quality assessments\n",
    "- **Confidence Intervals**: Provide uncertainty estimates with predictions\n",
    "\n",
    "## Usage in Production:\n",
    "\n",
    "```python\n",
    "# Load saved model\n",
    "model_data = joblib.load('models/qaqc_traditional_model.pkl')\n",
    "pipeline = QAQCPipeline(model_data['model'], model_data['scaler'], \n",
    "                       model_data['label_encoder'])\n",
    "\n",
    "# Assess new recording\n",
    "quality, confidence = pipeline.predict_quality('new_recording.wav')\n",
    "print(f\\\"Quality: {quality} (Confidence: {confidence:.3f})\\\")\n",
    "```\n",
    "\n",
    "This framework provides a solid foundation for building robust QAQC systems for medical audio applications."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
